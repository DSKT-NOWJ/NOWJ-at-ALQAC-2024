{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8878602,"sourceType":"datasetVersion","datasetId":5324275},{"sourceId":8927176,"sourceType":"datasetVersion","datasetId":5369854},{"sourceId":33551,"sourceType":"modelInstanceVersion","modelInstanceId":28083}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"code","source":"import json\nwith open('/kaggle/input/public-test-alqac1/law.json') as f:\n    law = json.load(f)\nwith open('/kaggle/input/public-test-alqac1/private_test_TASK_2.json') as f:\n    data = json.load(f)\nwith open('/kaggle/input/prompt-legal/prompts.json') as f:\n    prompts = json.load(f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompts['essay'] = [p + \"\\n\\nLưu ý: Chỉ cần trả lời đáp án, không cần giải thích\" for p in prompts['essay']]\nprompts['options'] = [p + \"\\n\\nLưu ý: Chỉ có 1 đáp án đúng và chỉ cần trả lời A, B, C hoặc D, không cần giải thích\" for p in prompts['options']]\nprompts['truefalse'] = [p + \"\\n\\nLưu ý: Chỉ cần trả lời Đúng hoặc Sai, không cần giải thích\" for p in prompts['truefalse']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_relevant_articles(relevant_articles, law_data):\n    articles_content = []\n    for i in range(len(relevant_articles)):\n        for law in law_data:\n            if law['id'] == relevant_articles[i]['law_id']:\n                for article in law['articles']:\n                    if article['id'] == relevant_articles[i]['article_id']:\n                        articles_content.append(article['text'])\n                        break\n#             raise ValueError('Article not found') \n    return \" \".join(articles_content)\nprint(get_relevant_articles(data[0]['relevant_articles'], law))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prompt_format_data(data, prompt):\n    if data['question_type'] == 'Trắc nghiệm':\n        return prompt.format(articles = get_relevant_articles(data['relevant_articles'],law), \n                    question = data['text'], choices = data['choices']) \n    else:\n        return prompt.format(articles = get_relevant_articles(data['relevant_articles'],law), \n                    question = data['text'])\nprint(prompt_format_data(data[92], prompts[\"essay\"][-1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LLAMA","metadata":{}},{"cell_type":"code","source":"!pip install -q groq","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport time\nfrom groq import Groq\n\nclient = Groq(\n    api_key='our-api-key'\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_answer_groq(prompt):\n    chat_completion = client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": prompt,\n            }\n        ],\n        model=\"llama3-70b-8192\",\n        temperature=0,\n        max_tokens=1024,\n    )\n\n    return chat_completion.choices[0].message.content","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"answer_llama = []\nfor i in range(0, len(data)):\n    prompt = \"\"\n    if data[i][\"question_type\"] == \"Đúng/Sai\":\n        prompt = prompt_format_data(data[i], prompts[\"truefalse\"][-1])\n    elif data[i][\"question_type\"] == \"Trắc nghiệm\":\n        prompt = prompt_format_data(data[i], prompts[\"options\"][-1])\n    else: \n        prompt = prompt_format_data(data[i], prompts[\"essay\"][-1])\n    response = generate_answer_groq(prompt)\n    print(data[i]['question_id'], ':', response) \n    answer_llama.append({\"question_id\": data[i]['question_id'],\n                        \"answer\": response})\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Gemini","metadata":{}},{"cell_type":"code","source":"pip install -q -U google-generativeai","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import google.generativeai as genai\nimport os\nimport time\ngenai.configure(api_key=\"our-api-key\")\n\nmodel = genai.GenerativeModel('gemini-1.5-flash')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = genai.GenerationConfig(\n    max_output_tokens=1024, temperature=0, top_p=1, top_k=32\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"answer_gemini = []\nfor i in range(0, len(data)):\n    prompt = \"\"\n    if data[i][\"question_type\"] == \"Đúng/Sai\":\n        prompt = prompt_format_data(data[i], prompts[\"truefalse\"][-1])\n    elif data[i][\"question_type\"] == \"Trắc nghiệm\":\n        prompt = prompt_format_data(data[i], prompts[\"options\"][-1])\n    else: \n        prompt = prompt_format_data(data[i], prompts[\"essay\"][-1])\n    response = model.generate_content(prompt, generation_config=config).text\n    print(data[i]['question_id'], ':', response) \n    answer_gemini.append({\"question_id\": data[i]['question_id'],\n                        \"answer\": response})\n    time.sleep(5)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# GPT","metadata":{}},{"cell_type":"code","source":"pip install openai","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"OPENAI_API_KEY='our-api-key'\nfrom openai import OpenAI\n\nclient = OpenAI(\n  api_key=OPENAI_API_KEY,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def completion_gpt(prompt):\n    completion = client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=[\n                {\"role\": \"user\", \"content\": prompt}\n        ], temperature=0\n    )\n    return str(completion.choices[0].message.content)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"answer_gpt = []\nfor i in range(0, len(data)):\n    prompt = \"\"\n    if data[i][\"question_type\"] == \"Đúng/Sai\":\n        prompt = prompt_format_data(data[i], prompts[\"truefalse\"][-1])\n    elif data[i][\"question_type\"] == \"Trắc nghiệm\":\n        prompt = prompt_format_data(data[i], prompts[\"options\"][-1])\n    else: \n        prompt = prompt_format_data(data[i], prompts[\"essay\"][-1])\n    response = completion_gpt(prompt)\n    print(data[i]['question_id'], ':', response) \n    answer_gpt.append({\"question_id\": data[i]['question_id'],\n                        \"answer\": response})\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Post processing","metadata":{}},{"cell_type":"code","source":"# format answer\ndef preprocessing(model_answer):\n    assert len(model_answer) == len(data)\n    for i in range(len(data)):\n        type_question = data[i]['question_type']\n        if type_question == 'Tự luận':\n            # replace '\\n' by ' ' -> strip\n            model_answer[i][\"answer\"] = model_answer[i][\"answer\"].replace('\\n', ' ').strip()\n\n        elif type_question == 'Trắc nghiệm':\n            # pass\n            # replace Đáp án: -> '',\n            model_answer[i][\"answer\"] = model_answer[i][\"answer\"].replace('Đáp án: ', '').strip()\n            model_answer[i][\"answer\"] = model_answer[i][\"answer\"].replace('Đáp án đúng là: ', '').strip()\n            model_answer[i][\"answer\"] = model_answer[i][\"answer\"].replace('**', '').strip()\n            model_answer[i][\"answer\"] = model_answer[i][\"answer\"].replace('Đáp án ', '').strip()\n            model_answer[i][\"answer\"] = model_answer[i][\"answer\"].replace('đúng là ', '').strip()\n        elif type_question == 'Đúng/Sai':\n            pass\n\n    return model_answer\n\nanswer_llama = preprocessing(answer_llama)\nanswer_gemini = preprocessing(answer_gemini)\nanswer_gpt = preprocessing(answer_gpt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get answer from answer format\n\ndef get_answer(model_answer_preprocess):\n    assert len(model_answer_preprocess) == len(data)\n    for i in range(len(data)):\n        type_question = data[i]['question_type']\n        if type_question == 'Đúng/Sai':\n            if model_answer_preprocess[i][\"answer\"].lower().__contains__('đúng'):\n                # data[i]['answer'] = \"Đúng\"\n                model_answer_preprocess[i][\"answer\"] = 'Đúng'\n            elif model_answer_preprocess[i][\"answer\"].lower().__contains__('sai'):\n                # data[i]['answer'] = \"Sai\"\n                model_answer_preprocess[i][\"answer\"] = 'Sai'\n            else:\n                print(model_answer_preprocess[i][\"answer\"])\n        elif type_question == 'Trắc nghiệm':\n            if model_answer_preprocess[i][\"answer\"].lower()[0] == 'a':\n                # data[i]['answer'] = 'A'\n                model_answer_preprocess[i][\"answer\"] = 'A'\n            elif model_answer_preprocess[i][\"answer\"].lower()[0] == 'b':\n                # data[i]['answer'] = 'B'\n                model_answer_preprocess[i][\"answer\"] = 'B'\n            elif model_answer_preprocess[i][\"answer\"].lower()[0] == 'c':\n                # data[i]['answer'] = 'C'\n                model_answer_preprocess[i][\"answer\"] = 'C'\n            elif model_answer_preprocess[i][\"answer\"].lower()[0] == 'd':\n                # data[i]['answer'] = 'D'\n                model_answer_preprocess[i][\"answer\"] = 'D'\n            else:\n                print(model_answer_preprocess[i][\"answer\"])\n                raise ValueError('Answer not found')\n            \n        elif type_question == 'Tự luận':\n            # data[i]['answer'] = model_answer_preprocess[i]\n            model_answer_preprocess[i][\"answer\"] = model_answer_preprocess[i][\"answer\"]\n\n    return model_answer_preprocess\n\nanswer_llama = get_answer(answer_llama)\nanswer_gemini = get_answer(answer_gemini)\nanswer_gpt = get_answer(answer_gpt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"answer_gpt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save to array","metadata":{}},{"cell_type":"code","source":"print(len(answer_llama))\nwith open('llama_v1.json', 'w', encoding='utf-8') as f:\n    json.dump(answer_llama, f, ensure_ascii=False, indent=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(answer_gemini))\nwith open('gemini_v1.json', 'w', encoding='utf-8') as f:\n    json.dump(answer_gemini, f, ensure_ascii=False, indent=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(answer_gpt))\nwith open('gpt_v1.json', 'w', encoding='utf-8') as f:\n    json.dump(answer_gpt, f, ensure_ascii=False, indent=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble","metadata":{}},{"cell_type":"code","source":"with open('llama_v1.json') as f:\n    model_1 = json.load(f)\nwith open('gemini_v1.json') as f:\n    model_2 = json.load(f)\nwith open('gpt_v1.json') as f:\n    model_3 = json.load(f)\n    \n# If you want to ensemble more result files, the code to read additional files is here.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_answers = [model_1, model_2, model_3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_answers = []\nfor i in range(len(list_answers[0])):\n    if \"TL\" in list_answers[0][i][\"question_id\"]:\n        new_answers.append({\"question_id\": list_answers[0][i][\"question_id\"],\n                          \"answer\": list_answers[2][i][\"answer\"]})\n    elif \"DS\" in answers[0][i][\"question_id\"]:\n        answer = [list_answers[j][i][\"answer\"] for j in range(len(list_answers))]\n        num_y = answer.count('Đúng')\n        num_n = answer.count('Sai')\n        if num_y > num_n:\n            new_answers.append({\"question_id\": list_answers[0][i][\"question_id\"],\n                          \"answer\": 'Đúng'})\n        else:\n            new_answers.append({\"question_id\": list_answers[0][i][\"question_id\"],\n                          \"answer\": 'Sai'})\n    else:\n        answer = [list_answers[j][i][\"answer\"] for j in range(len(list_answers))]\n        num_A = answer.count('A')\n        num_B = answer.count('B')\n        num_C = answer.count('C')\n        num_D = answer.count('D')\n        mx = max([num_A, num_B, num_C, num_D])\n        if mx == num_A:\n            new_answers.append({\"question_id\": list_answers[0][i][\"question_id\"],\n                          \"answer\": 'A'})\n        elif mx == num_B:\n            new_answers.append({\"question_id\": list_answers[0][i][\"question_id\"],\n                          \"answer\": 'B'})\n        elif mx == num_C:\n            new_answers.append({\"question_id\": list_answers[0][i][\"question_id\"],\n                          \"answer\": 'C'})\n        elif mx == num_D:\n            new_answers.append({\"question_id\": list_answers[0][i][\"question_id\"],\n                          \"answer\": 'D'})\nlen(list_answers[0]), len(new_answers)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('ensemble.json', 'w', encoding='utf-8') as f:\n    json.dump(new_answers, f, ensure_ascii=False, indent=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}